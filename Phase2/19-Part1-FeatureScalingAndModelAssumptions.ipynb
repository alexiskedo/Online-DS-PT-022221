{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression, Part 1: \n",
    "\n",
    "## Feature Scaling and Model Assumptions\n",
    "\n",
    "AKA - Topic 19 is too big to fit into one study group! We'll do part 1 today, focused on how we can better interpret the results of a linear regression model that includes multiple features, as well as some things to check to make sure our models are reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First: Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit data from https://www.kaggle.com/avikpaul4u/credit-card-balance\n",
    "\n",
    "Target: `Balance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "df = pd.read_csv('data/Credit.csv', \n",
    "                 usecols=['Income', 'Limit', 'Rating', 'Age', 'Balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Same as simple linear regression, but with more inputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with statsmodels\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our X and y\n",
    "\n",
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at our results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation time!\n",
    "\n",
    "How'd we do? What looks different from the simple linear regression output? What in the world can we do with those coefficients?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization, AKA Feature Scaling and Centering\n",
    "\n",
    "Scaling data is the process of **increasing or decreasing the magnitude according to a fixed ratio.** You change the size but not the shape of the data. Often, this involves divivding features by their standard deviation.\n",
    "\n",
    "Centering also does not change the shape of the data, but instead simply **removes the mean value  of each feature** so that each is centered around zero instead of their original mean.\n",
    "\n",
    "The idea is that you can standardize data to be comparable, so that a model can interpret each individual feature more consistently.\n",
    "\n",
    "**NOTE:** Feature Scaling is **NOT** the same as normalization!\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Standard Scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "The most common method of scaling is standardization.  In this method we center the data, then we divide by the standard devation to enforce that the standard deviation of the variable is one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [MinMax Scalar](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "\n",
    "From the documentation:\n",
    "\n",
    "> This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Robust Scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)\n",
    "\n",
    "From the documentation:\n",
    "\n",
    "> Scale features using statistics that are robust to outliers.\n",
    ">\n",
    "> This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n",
    "\n",
    "Aka like a standard scaler, but uses median and IQR variance instead of mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing some options so we can check out the differences between them\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating our different scalers\n",
    "stdscaler = StandardScaler()\n",
    "minmaxscaler = MinMaxScaler()\n",
    "robscaler = RobustScaler()\n",
    "\n",
    "# Creating scaled versions of one column\n",
    "X_scaled_std = stdscaler.fit_transform(X['Age'].values.reshape(-1, 1))\n",
    "X_scaled_mm = minmaxscaler.fit_transform(X['Age'].values.reshape(-1, 1))\n",
    "X_scaled_rob = robscaler.fit_transform(X['Age'].values.reshape(-1, 1))\n",
    "# why fit_transform? We'll discuss in a second\n",
    "\n",
    "# defining a dictionary of these things to better visualize\n",
    "scalers = {'Original': X['Age'].values, \n",
    "           'Standard Scaler': X_scaled_std, \n",
    "           'Min Max Scaler': X_scaled_mm,\n",
    "           'Robust Scaler': X_scaled_rob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize it!\n",
    "for title, data in scalers.items():\n",
    "    plt.hist(data, bins=20)\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you thinking to yourself: \"Hey - these all look the same.\"\n",
    "\n",
    "**EXACTLY**\n",
    "\n",
    "The point is that you change the scale on the X axis WITHOUT changing the shape of the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need to use feature scaling?\n",
    "\n",
    "- In order to compare the magnitude of coefficients thus increasing the interpretability of coefficients\n",
    "- Handling disparities in units\n",
    "- Some models use euclidean distance in their computations\n",
    "- Some models require features to be on equivalent scales\n",
    "- In the machine learning space, it helps improve the performance of the model and reducing the values/models from varying widely\n",
    "- Some algorithms are sensitive to the scale of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Try It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nstantiate a scaler and scale our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can transform that from an array to a df to explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now model on the scaled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at our results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate:\n",
    "\n",
    "What changed?\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "### Now that we're ready to look at our inputs - how do we interpret these coefficients, or those p-values in the summary?\n",
    "\n",
    "Discuss:\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "See anything you wouldn't expect? Could be because we haven't yet made our models reliable - aren't adhering to the assumptions of linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Assumptions of Linear Regression\n",
    "\n",
    "Linear regression models have some underlying assumptions, mostly captured by the following points:\n",
    "\n",
    "1. The true relationship is linear\n",
    "2. No multicollinearity between independent variables\n",
    "3. Errors are normally distributed with mean 0\n",
    "4. Errors are homoskedastic (aka they have constant variance)\n",
    "5. Errors are not correlated (no trends in error terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![the office \"you're making some very dangerous assumptions\" gif from gfycat](https://thumbs.gfycat.com/DarkParallelArizonaalligatorlizard-size_restricted.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Each Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity\n",
    "\n",
    "Why do we assume linearity? Because, by modeling the relationship using _linear_ regression - if we don't think the relationship is linear, we probably should use a different model.\n",
    "\n",
    "I'll note that linear regression can still handle curvature in the relationship using polynomial variables, interaction terms, etc (more on that in Topic 20) - but this assumption captures the idea that linear parameters (aka coefficients) can capture the relationship between X and y.\n",
    "\n",
    "This assumption can be checked by using scatterplots - plotting the dependent variable against every independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_cols:\n",
    "    plt.scatter(df[x], df['Balance'])\n",
    "    plt.title(f'Plot of Balance against {x}')\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel('Balance')\n",
    "    plt.show()\n",
    "    \n",
    "# also plot sales against itself\n",
    "plt.scatter(df.index, df['Balance'])\n",
    "plt.hlines(df['Balance'].mean(), 0, df.index.max(), color='r')\n",
    "plt.xlabel('Index Value')\n",
    "plt.ylabel('Balance')\n",
    "plt.title('Variance of Balance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quicker solution is to use Seaborn's `pairplot`.  \n",
    "\n",
    "This lets us check for linearity and multicollinearity (the next assumption we'll check) at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Pearson's R Value\n",
    "\n",
    "Pearson's R represents a correlation coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check correlations just against sales\n",
    "df.corr().Balance.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "\n",
    "AKA when my X variables aren't actually independent - so that a model has trouble determining which change in what X variable is actually influencing `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directly Explore Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check all correlations using the same Pearson's correlation coefficient\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also visualize it\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Variance Inflation Factor (VIF)\n",
    "\n",
    "> \"Variance inflation factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables. Mathematically, the VIF for a regression model variable is **equal to the ratio of the overall model variance to the variance of a model that includes only that single independent variable**. This ratio is calculated for each independent variable. A high VIF indicates that the associated independent variable is highly collinear with the other variables in the model.\"\n",
    "\n",
    "-- Source: https://www.investopedia.com/terms/v/variance-inflation-factor.asp\n",
    "\n",
    "In other words - how well does one of these X variables predict the others?\n",
    "\n",
    "reference: https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# defining a dataframe with just our X variables\n",
    "X = df[X_cols]\n",
    "\n",
    "# defining an empty dataframe to capture the VIF scores\n",
    "vif = pd.DataFrame()\n",
    "\n",
    "# For each column,run a variance_inflaction_factor against all other columns to get a VIF Factor score\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "\n",
    "# label the scores with their related columns\n",
    "vif[\"features\"] = df[X_cols].columns\n",
    "\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Residuals\n",
    "\n",
    "By checking the residuals, aka the error between our actual y values and what we predicted, we can check that:\n",
    "\n",
    "- Errors are normally distributed with mean 0\n",
    "- Errors are homoskedastic (aka they have constant variance)\n",
    "- Errors are not correlated (no trends in error terms)\n",
    "\n",
    "In a nutshell:\n",
    "\n",
    "<img src=\"images/error-dist.jpeg\" width=\"550\">  \n",
    "\n",
    "\n",
    "#### Typical Residuals vs. Predictions plots:\n",
    "\n",
    "- **The ideal scenario**\n",
    "\n",
    "    - Random scatter\n",
    "    - Scattered around 0\n",
    "    - No identifiable trend\n",
    "    \n",
    "    <img src=\"images/normal-resid.png\" width=\"550\">  \n",
    "    \n",
    "- **Non-linear relationship**\n",
    "\n",
    "    - Clear non-linear scatter, but\n",
    "    - Identifiable trend\n",
    "    - **Fix:** Introduce polynomial terms\n",
    "    - **Fix:** Variable transformation\n",
    "    \n",
    "    <img src=\"images/polynomial-resid.png\" width=\"550\">\n",
    "\n",
    "- **Autocorrelation**\n",
    "\n",
    "    - Identifiable trend, or\n",
    "    - Consecutively positive/negative residuals\n",
    "    - **Fix:** Consider sequential analysis methods (which we'll discuss in phase 4)\n",
    "    \n",
    "    <img src=\"images/autocorrelation.png\" width=\"550\">\n",
    "\n",
    "- **Heteroskedasticity**\n",
    "\n",
    "    - The spread of residuals is different at different levels of the fitted values\n",
    "    - **Fix:** Variable transformation (log)  \n",
    "    \n",
    "    <img src=\"images/heteroskedasticity.png\" width=\"550\">\n",
    "    \n",
    "The above plots were created using `seaborn.residplot` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X, y)\n",
    "\n",
    "preds = lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality\n",
    "\n",
    "There are several ways to test for normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y - preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(residuals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ plots are generally great tools for checking for normality.\n",
    "import statsmodels.api as sm\n",
    "\n",
    "fig = sm.qqplot(residuals, line = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're just doing/exploring simple linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that these residplots only work for single variables\n",
    "\n",
    "for x in X_cols:\n",
    "    sns.residplot(x, \"Balance\", data=df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our full model\n",
    "plt.scatter(preds, residuals)\n",
    "plt.axhline(y=0, color = 'red', label = '0')\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Potential Problems\n",
    "\n",
    "- Outliers\n",
    "\n",
    "    <img src='images/outliers.png' width=450>\n",
    "\n",
    "- High Leverage Points \n",
    "\n",
    "    <img src='images/leverage.png' width=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So Let's Fix It!\n",
    "\n",
    "What problems have we identified?\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "How can we potentially fix them?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Is Part 1....\n",
    "\n",
    "In Part 2, we'll cover how to validate our models, aka how to make sure they generalize well to unseen data. We'll also talk about the importance of _training_ our models only on the data that we want it to see!\n",
    "\n",
    "We'll table how to transform categorical variables to be able to be used in models, and cover that with Polynomial Regression in our Topic 20 study group!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "\n",
    "- [Excellent statistical writeup about how to interpret Linear Regression coefficients, and their p-values](https://statisticsbyjim.com/regression/interpret-coefficients-p-values-regression/)\n",
    "\n",
    "- [Detecting Multicollinearity with VIF](https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/)\n",
    "\n",
    "- [Penn State Stats on Influential Points (outliers, high leverage points)](https://online.stat.psu.edu/stat462/node/87/) - this resource also allows easy access to the rest of their material on regression\n",
    "\n",
    "- [Statsmodels' Documentation: Check the influence of outliers](https://www.statsmodels.org/devel/generated/statsmodels.stats.outliers_influence.OLSInfluence.html)\n",
    "\n",
    "- [Long blog post on regression diagnostics with implementation in python](http://songhuiming.github.io/pages/2016/12/31/linear-regression-in-python-chapter-2/)\n",
    "\n",
    "- [Statistics by Jim: Linear Regression Assumptions](https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
